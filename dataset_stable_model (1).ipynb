{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "dataset   stable_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zgFCEmb-VNL9",
        "vGg_W977yTMT",
        "4YiJqKm3KvAP",
        "tax0XUemKnii",
        "WulvcEixMx_n",
        "FO82c2npM9Iq",
        "fZRGgCV_7wAB",
        "w8F028jMNHl4",
        "xO6Du6Imhj8K",
        "64QUAifGhsxR",
        "vJfGJDUKS2WO",
        "les4y7T5NKaC",
        "dn5y2GCDNhU_",
        "pBNYPClR7mRo",
        "L74F3rcQHZpv",
        "Zz_W2f5n7fC6",
        "muFsIeJIFvkG",
        "OW7JiiOQDyof",
        "4Faa2vji5Oym",
        "GrqoDMFY54u7",
        "xlGc2mE86l0F",
        "fbmMwYjFuW2x",
        "ggfSXSwd03Fl",
        "ZcMjJexM3PIc",
        "0KAreNTvnm5Y"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgFCEmb-VNL9"
      },
      "source": [
        "###Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s5OLDamVRz0"
      },
      "source": [
        "\n",
        "Weeks :\n",
        "\n",
        "1.   Setup a working NN w/  a working fiting loop\n",
        "2.   Completing the fiting loop & adding the contrastive loop\n",
        "3.   A lot of tests to figure out why it's not great\n",
        "4.   Revisiting the custom loss function\n",
        "5.   Accelerating the loop & NN & start a lot of tests\n",
        "6.   Tests & trying to optimize the performances of the method\n",
        "7.   Tests, performances & starting to tidy up the code\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slm2TO0O_m3O"
      },
      "source": [
        "change the projection head\\\n",
        "Verify the loss function, maybe add several losses\n",
        "\n",
        "\n",
        "17/07 morning : \n",
        "I don't really understand anything at this point, resnet seem way worse than the little convolutive nn that I did EVEN THOUGH IT IS ALREADY TRAINED. With transfer learning I only got to get around 30% accuracy idk why. I didn't even used custom functions for that one. My guess is that transfer learning is very hard with that small of a dataset. But it would be weird to still get easily 60% acc with my convolutive NN, resnet50 is just harder to train than my small nn\n",
        "I also need to recheck widenet and establish a sheet with all the test that I can / have to do\n",
        "\n",
        "Start week 6\n",
        "I have no idea why but the loss function given in the github seem to work way better than mine. I don't understand how this funciton work and I'm pretty sure mine fit the loss function given in the paper.\n",
        "Test 16 shows that the github_contrastive_loss decrease very little over the epochs\\\n",
        "21/07 We decided to check on the full dataset the regular fit to debug, on the whole cnn we can achieve 64% acc decreasing to 58% when we freeze the body.\\\n",
        "Same experience on wideresnet from 76% acc to 60% acc\n",
        "23/07 With WideResNet the overfitting is incredible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGg_W977yTMT"
      },
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnzTziLU1Yet",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7db47c10-109e-4372-e092-1af8fae87233"
      },
      "source": [
        "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir\n",
        "!pip install keras --upgrade\n",
        "!pip install -Uqq fastbook\n",
        "!pip install -q tf-nightly-2.0-preview\n",
        "!pip install XlsxWriter\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/albu/albumentations\n",
            "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-xx8nuuw5\n",
            "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-xx8nuuw5\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (1.4.1)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (0.16.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (3.13)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==1.1.0) (4.1.2.30)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations==1.1.0) (3.7.4.3)\n",
            "Collecting opencv-python-headless>=4.0.1\n",
            "  Downloading opencv_python_headless-4.5.4.58-cp37-cp37m-manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations==1.1.0) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.1.0) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.1.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.1.0) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.1.0) (1.0.1)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-1.1.0-py3-none-any.whl size=103057 sha256=f83908393f040758e4fa1db9203dcbe8b6cf2d7da4071543de0d29ceadc652ba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-q0k2cbyv/wheels/63/11/1a/c77caf3ae9b9b6d57b3ee5e6a41a50f3bc12c66a70f6b90bf0\n",
            "Successfully built albumentations\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.4.58 qudida-0.0.4\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.6.0)\n",
            "Collecting keras\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 5.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.6.0\n",
            "    Uninstalling keras-2.6.0:\n",
            "      Successfully uninstalled keras-2.6.0\n",
            "Successfully installed keras-2.7.0\n",
            "\u001b[K     |████████████████████████████████| 720 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 189 kB 41.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 32.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 51 kB 287 kB/s \n",
            "\u001b[?25h\u001b[31mERROR: Could not find a version that satisfies the requirement tf-nightly-2.0-preview (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for tf-nightly-2.0-preview\u001b[0m\n",
            "Collecting XlsxWriter\n",
            "  Downloading XlsxWriter-3.0.2-py3-none-any.whl (149 kB)\n",
            "\u001b[K     |████████████████████████████████| 149 kB 5.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: XlsxWriter\n",
            "Successfully installed XlsxWriter-3.0.2\n",
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-0tdcwu_l\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-0tdcwu_l\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from keras-contrib==2.0.8) (2.7.0)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-py3-none-any.whl size=101077 sha256=fda57979d518ef391e07c420053f10944a332fc8cc632a21a08e34086dcc58cb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d3tla2jt/wheels/bb/1f/f2/b57495012683b6b20bbae94a3915ec79753111452d79886abc\n",
            "Successfully built keras-contrib\n",
            "Installing collected packages: keras-contrib\n",
            "Successfully installed keras-contrib-2.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YiJqKm3KvAP"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqM-hPs-AH16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "489e16a4-c8d4-4190-e03c-bce532b68f35"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tfc\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.utils import to_categorical # The neural networks are made and trained using tensorflow\n",
        "\n",
        "import tensorflow_datasets as tfds # To manipulate datasets\n",
        "\n",
        "import albumentations as alb # For data augmentation\n",
        "\n",
        "import xlsxwriter as xw\n",
        "from xlsxwriter.utility import xl_rowcol_to_cell\n",
        "import pandas as pd # To make xls file recording trainings\n",
        "\n",
        "\n",
        "# from keras_contrib.optimizers import LARS \n",
        "# This optimiser is not callable in eager execution\n",
        "\n",
        "import numpy as np\n",
        "from math import log, exp, ceil\n",
        "import matplotlib.pyplot as plt # Math & plot\n",
        "\n",
        "\n",
        "from datetime import datetime, timezone, timedelta\n",
        "import pytz\n",
        "now = datetime.now()\n",
        "tz = pytz.timezone(\"Europe/Paris\") # To add metadata to the xls test sheets\n",
        "\n",
        "from time import sleep, time # To test the speed of my functions\n",
        "\n",
        "from tqdm.notebook import tqdm, tqdm_notebook, trange # To add loading bars to the training loop, it's very usefull since the training time can be quite long and varies a lot\n",
        "\n",
        "import os \n",
        "import fastbook as fb# Save/load xls files and neural networks\n",
        "\n",
        "\n",
        "fb.setup_book()\n",
        "\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y\")\n",
        "PATH = str(fb.gdrive)+\"/\"+date+\"/\"\n",
        "if not os.path.exists(PATH):\n",
        "  os.mkdir(PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AlreadyExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8da726bb94b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/api/_v2/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Generic layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_loss_scale_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madadelta\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madadelta_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madagrad\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madagrad_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madam_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adadelta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n\u001b[0;32m---> 37\u001b[0;31m     \"/tensorflow/api/keras/optimizers\", \"keras optimizer usage\", \"method\")\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m _DEFAULT_VALID_DTYPES = frozenset([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/monitoring.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, description, *labels)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \"\"\"\n\u001b[1;32m    360\u001b[0m     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\n\u001b[0;32m--> 361\u001b[0;31m                                     len(labels), name, description, *labels)\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/monitoring.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m           self._metric_name, len(self._metric_methods)))\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tax0XUemKnii"
      },
      "source": [
        "#Main code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulvcEixMx_n"
      },
      "source": [
        "## Import Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9e91KYZPMHv"
      },
      "source": [
        "On the final version we're using the cifar10 dataset as tensorflow_dataset's datasets. I tried several module to manipulate datasets and this one seems the best I found for what I needed. Ineed, I needed a precise manipulation of the individual elements of the dataset since the contrastive learning require a pairing of each augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpEikF384PZb"
      },
      "source": [
        "def len_ds(ds : tf.data.Dataset) -> int :\n",
        "  \"\"\"\n",
        "  Compute the length of a dataset, most often the number of batches if the dataset is correctly built.\n",
        "  This function is computationnaly expensive and should only be used for debugging\n",
        "  \n",
        "  Args :\n",
        "    ds : A dataset\n",
        "\n",
        "  Return : \n",
        "    The lenght of the dataset\n",
        "\n",
        "  \"\"\"\n",
        "  out = 0\n",
        "  for _ in ds :\n",
        "    out += 1\n",
        "  return out\n",
        "\n",
        "def plot_batch(batch_x : tf.Tensor, title : str = \"\") :\n",
        "  \"\"\" Plot a batch\n",
        "  Plot every images in a batch  in 4 columns\n",
        "\n",
        "  Args :\n",
        "    batch_x : a batch / set of images to plot\n",
        "    title : The title of the plot\n",
        "  \n",
        "  Returns :\n",
        "    Nothing\n",
        "  \"\"\"\n",
        "  columns = 4\n",
        "  rows = len(batch_x)//4 +1\n",
        "  fig = plt.figure(figsize=(30, 30))\n",
        "  plt.title(title)\n",
        "  for i, img in enumerate(batch_x) :\n",
        "    fig.add_subplot(rows, columns, i+1)\n",
        "    plt.imshow(img)\n",
        "\n",
        "def format_image(image : tf.Tensor, label : tf.Tensor = None) -> tf.Tensor :\n",
        "  \"\"\" Format an image\n",
        "  Transform an image in format [0,255] to a format [0,1] \n",
        "  making it easier to process in machine learning\n",
        "\n",
        "  Args : \n",
        "    image : The image to format\n",
        "  \n",
        "  Returns : \n",
        "    formated image\n",
        "  \"\"\"\n",
        "  if label == None :\n",
        "    return image/255\n",
        "  else :\n",
        "    return image/255, label\n",
        "\n",
        "def normalize_cifar10_element(image : tf.Tensor, label : tf.Tensor = None) -> tf.Tensor :\n",
        "  \"\"\" Normalize an image from cifar10\n",
        "  Normalize an image from cifar10 according to the mean and standard deviation of the cifar10 dataset\n",
        "\n",
        "  Args :\n",
        "    image : The image to normalize\n",
        "\n",
        "  returns :\n",
        "    normalized image \n",
        "  \"\"\"\n",
        "  if label == None :\n",
        "    return (image -(0.4914, 0.4822, 0.4465))/(0.247, 0.243, 0.261)\n",
        "  else :\n",
        "    return (image -(0.4914, 0.4822, 0.4465))/(0.247, 0.243, 0.261),label\n",
        "\n",
        "def format_batch(batchx : tf.Tensor, batchy : tf.Tensor = None):\n",
        "  \"\"\" Format a batch\n",
        "  Format a batch using format image\n",
        "  can be used with the features tensor or a batch containing features and labels\n",
        "\n",
        "  Args : \n",
        "    batchx : Features\n",
        "    batchy : Labels\n",
        "\n",
        "  returns :\n",
        "    formated batch\n",
        "  \"\"\"\n",
        "  if batchy == None :\n",
        "    return tf.map_fn(format_image, batchx, fn_output_signature= tf.float32)\n",
        "  else :\n",
        "    return tf.map_fn(format_image, batchx, fn_output_signature= tf.float32), batchy\n",
        "\n",
        "def get_x( x : tf.Tensor, y : tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\" get features from a batch\n",
        "\n",
        "  Args : \n",
        "    x : Features\n",
        "    y : Labels\n",
        "  \n",
        "  Returns :\n",
        "    Features\n",
        "  \"\"\"\n",
        "  return x\n",
        "\n",
        "def get_y( x : tf.Tensor, y : tf.Tensor) -> tf.Tensor:\n",
        "  \"\"\" get labels from a batch\n",
        "\n",
        "  Args : \n",
        "    x : Features\n",
        "    y : Labels\n",
        "  \n",
        "  Returns :\n",
        "    Labels\n",
        "  \"\"\"\n",
        "  return y\n",
        "\n",
        "def shape(ds : tf.data.Dataset) -> tf.Tensor :\n",
        "  \"\"\" Shape of a dataset\n",
        "\n",
        "  Args : \n",
        "    ds : Dataset\n",
        "\n",
        "  Returns :\n",
        "    the shape\n",
        "  \"\"\"\n",
        "  dataset_to_numpy = list(ds.as_numpy_iterator())\n",
        "  shape = tf.shape(dataset_to_numpy)\n",
        "  return shape\n",
        "\n",
        "def from_int_to_categorical(x,y, num_classes) :\n",
        "  \"\"\" Convert labels from a batch from numerical to categorical\n",
        "  \n",
        "  Example : \n",
        "    a labels tensor [0,2,1] with num_classes = 3 will become \n",
        "    [[1,0,0],\n",
        "     [0,0,1],\n",
        "     [0,1,0]]\n",
        "    \n",
        "    Args : \n",
        "      x : Features\n",
        "      y : Labels\n",
        "      num_classes : The number of classes\n",
        "\n",
        "    Returns : \n",
        "      A batch with labels formated\n",
        "  \"\"\"\n",
        "  return x, one_hot(y,num_classes)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWY0QHk3iIeq"
      },
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "num_classes = 10\n",
        "batch_size = 50\n",
        "\n",
        "def generate_datasets(bank = \"cifar10\", num_classes = 10) :\n",
        "  \"\"\" Generate and format the datasets needed for SimCLR \n",
        "  SimCLR is a semi-supervised learning method, so we have a big unlabeled dataset and a small labeled dataset\n",
        "  the small labeled dataset is split in three parts respectively the training dataset, the test datatest and the validation datatest\n",
        "\n",
        "  Args : \n",
        "    bank : The name of the dataset, it must be a dataset that can be loaded by tensorflow_datasets\n",
        "    num_classes : The number of classes in the dataset\n",
        "  \n",
        "  Returns :\n",
        "    Unlabeled dataset\n",
        "    Train dataset\n",
        "    Test dataset\n",
        "    Validation dataset\n",
        "  \n",
        "  \"\"\"\n",
        "  # Load the datasets\n",
        "  unlabeled_ds = tfds.load(bank, split='train', as_supervised=True)\n",
        "  train_ds, test_ds, validation_ds = tfds.load(bank, split=['test[0%:80%]','test[80%:90%]','test[90%:100%]'],as_supervised=True, batch_size=batch_size)\n",
        "  # Get rid of the labels for the unlabeled dataset, format and prefect the dataset\n",
        "  unlabeled_ds = unlabeled_ds.map(get_x)\n",
        "  unlabeled_ds = unlabeled_ds.map(format_image, num_parallel_calls=AUTOTUNE)\n",
        "  unlabeled_ds = unlabeled_ds.prefetch(AUTOTUNE)\n",
        "  # Format the labeled datasets\n",
        "  train_ds = train_ds.map(format_image,  num_parallel_calls=AUTOTUNE)\n",
        "  test_ds = test_ds.map(format_image,  num_parallel_calls=AUTOTUNE)\n",
        "  validation_ds = validation_ds.map(format_image,  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # Normalize the labeled datasets\n",
        "  train_ds = train_ds.map(normalize_cifar10_element,  num_parallel_calls=AUTOTUNE)\n",
        "  test_ds = test_ds.map(normalize_cifar10_element,  num_parallel_calls=AUTOTUNE)\n",
        "  validation_ds = validation_ds.map(normalize_cifar10_element,  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "  # Format the labeled datasets\n",
        "  train_ds = train_ds.map(lambda x,y: from_int_to_categorical(x,y,num_classes))\n",
        "  test_ds = test_ds.map(lambda x,y: from_int_to_categorical(x,y,num_classes))\n",
        "  validation_ds = validation_ds.map(lambda x,y: from_int_to_categorical(x,y,num_classes))\n",
        "\n",
        "  # Prefecth the labeled datasets\n",
        "  train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "  test_ds = test_ds.prefetch(AUTOTUNE)\n",
        "  validation_ds = validation_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "  return unlabeled_ds, train_ds, test_ds, validation_ds\n",
        "\n",
        "\n",
        "#def get_generators()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO82c2npM9Iq"
      },
      "source": [
        "## Pre-build models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CwhuECWQYpD"
      },
      "source": [
        "The architecture of the neural networks used in this project is not an object.\n",
        "\n",
        "For this reason I chose to have a simple and fast to train neural network in a first time and then to import a ResNet50 as mentionned in the SimCLR paper for the performance tests. I also imported another neural network, the WideNet to try if that would change the performances.\n",
        "\n",
        "Another thing to note is that the neural networks are separated in two parts since a projection head is attached to the nn during the training and is swapped for a classifier during the testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZRGgCV_7wAB"
      },
      "source": [
        "### Simple Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkI7SGFwNBBK"
      },
      "source": [
        "def build_simple_conv() :\n",
        "  \"\"\"\n",
        "  Build a simple convolutive neural network, it needs a classifier to work \n",
        "  \n",
        "  Returns :\n",
        "    Convolutive body\n",
        "  \"\"\"\n",
        "  model = Sequential(name = \"simple_conv_network\")\n",
        "\n",
        "  model.add(Conv2D(64,(3,3),input_shape = (32,32,3), activation='relu', dtype='float32', name='input_image',kernel_initializer='random_normal',bias_initializer='zeros'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(128,(3,3), activation='relu', padding='same',kernel_initializer='random_normal',bias_initializer='zeros'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(256,(3,3), activation='relu', padding='same',kernel_initializer='random_normal',bias_initializer='zeros'))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def build_resnet():\n",
        "  \"\"\"\n",
        "  Build a resnet from keras\n",
        "  \"\"\"\n",
        "  kerasResNet50 = keras.applications.ResNet50(include_top=False,\n",
        "                                        weights='imagenet',\n",
        "                                      input_tensor=None,\n",
        "                                      input_shape=(32,32,3),\n",
        "                                      pooling = 'max',\n",
        "                                      classes = num_classes)\n",
        "\n",
        "  return kerasResNet50\n",
        "\n",
        "def build_projection_head():\n",
        "  \"\"\"\n",
        "  Build a projection head\n",
        "  \"\"\"\n",
        "  projection_head = Sequential([\n",
        "                            Dense(128, activation='relu',kernel_initializer='random_normal',bias_initializer='zeros'),\n",
        "                            Dense(128, activation='relu',kernel_initializer='random_normal',bias_initializer='zeros')\n",
        "  ],name = \"Projection_head\")\n",
        "  return projection_head\n",
        "\n",
        "\n",
        "\n",
        "def build_classifier():\n",
        "  \"\"\"\n",
        "  Build a linear classifier\n",
        "  \"\"\"\n",
        "  classifier = Sequential([\n",
        "                            #Dense(2048, activation='relu'),\n",
        "                            Dense(num_classes, activation='softmax')\n",
        "  ],name = \"linear_classifier\")\n",
        "  return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuoxBFgj7zNu"
      },
      "source": [
        "### WideNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lcXNfiZ8DGw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "881399b8-76f4-49fa-a781-df144000588e"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Conv2D, Dense, Input, add, Activation, Flatten, AveragePooling2D\n",
        "from keras.callbacks import LearningRateScheduler, TensorBoard\n",
        "from keras.regularizers import l2\n",
        "from keras import optimizers\n",
        "from keras.models import Model\n",
        "\n",
        "DEPTH              = 28\n",
        "WIDE               = 2\n",
        "IN_FILTERS         = 16\n",
        "\n",
        "CLASS_NUM          = 10\n",
        "IMG_ROWS, IMG_COLS = 32, 32\n",
        "IMG_CHANNELS       = 3\n",
        "\n",
        "BATCH_SIZE         = 128\n",
        "EPOCHS             = 200\n",
        "ITERATIONS         = 50000 // BATCH_SIZE + 1\n",
        "WEIGHT_DECAY       = 0.0005\n",
        "LOG_FILE_PATH      = './w_resnet/'\n",
        "\n",
        "\n",
        "def scheduler(epoch):\n",
        "    if epoch < 60:\n",
        "        return 0.1\n",
        "    if epoch < 120:\n",
        "        return 0.02\n",
        "    if epoch < 160:\n",
        "        return 0.004\n",
        "    return 0.0008\n",
        "\n",
        "def color_preprocessing(x_train,x_test):\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    mean = [125.3, 123.0, 113.9]\n",
        "    std  = [63.0,  62.1,  66.7]\n",
        "    for i in range(3):\n",
        "        x_train[:,:,:,i] = (x_train[:,:,:,i] - mean[i]) / std[i]\n",
        "        x_test[:,:,:,i] = (x_test[:,:,:,i] - mean[i]) / std[i]\n",
        "\n",
        "    return x_train, x_test\n",
        "\n",
        "def wide_residual_network(img_input,classes_num,depth,k):\n",
        "    print('Wide-Resnet %dx%d' %(depth, k))\n",
        "    n_filters  = [16, 16*k, 32*k, 64*k]\n",
        "    n_stack    = (depth - 4) // 6\n",
        "\n",
        "    def conv3x3(x,filters):\n",
        "        return Conv2D(filters=filters, kernel_size=(3,3), strides=(1,1), padding='same',\n",
        "        kernel_initializer='he_normal',\n",
        "        kernel_regularizer=l2(WEIGHT_DECAY),\n",
        "        use_bias=False)(x)\n",
        "\n",
        "    def bn_relu(x):\n",
        "        x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
        "        x = Activation('relu')(x)\n",
        "        return x\n",
        "\n",
        "    def residual_block(x,out_filters,increase=False):\n",
        "        global IN_FILTERS\n",
        "        stride = (1,1)\n",
        "        if increase:\n",
        "            stride = (2,2)\n",
        "            \n",
        "        o1 = bn_relu(x)\n",
        "        \n",
        "        conv_1 = Conv2D(out_filters,\n",
        "            kernel_size=(3,3),strides=stride,padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            kernel_regularizer=l2(WEIGHT_DECAY),\n",
        "            use_bias=False)(o1)\n",
        "\n",
        "        o2 = bn_relu(conv_1)\n",
        "        \n",
        "        conv_2 = Conv2D(out_filters, \n",
        "            kernel_size=(3,3), strides=(1,1), padding='same',\n",
        "            kernel_initializer='he_normal',\n",
        "            kernel_regularizer=l2(WEIGHT_DECAY),\n",
        "            use_bias=False)(o2)\n",
        "        if increase or IN_FILTERS != out_filters:\n",
        "            proj = Conv2D(out_filters,\n",
        "                                kernel_size=(1,1),strides=stride,padding='same',\n",
        "                                kernel_initializer='he_normal',\n",
        "                                kernel_regularizer=l2(WEIGHT_DECAY),\n",
        "                                use_bias=False)(o1)\n",
        "            block = add([conv_2, proj])\n",
        "        else:\n",
        "            block = add([conv_2,x])\n",
        "        return block\n",
        "\n",
        "    def wide_residual_layer(x,out_filters,increase=False):\n",
        "        global IN_FILTERS\n",
        "        x = residual_block(x,out_filters,increase)\n",
        "        IN_FILTERS = out_filters\n",
        "        for _ in range(1,int(n_stack)):\n",
        "            x = residual_block(x,out_filters)\n",
        "        return x\n",
        "\n",
        "\n",
        "    x = conv3x3(img_input,n_filters[0])\n",
        "    x = wide_residual_layer(x,n_filters[1])\n",
        "    x = wide_residual_layer(x,n_filters[2],increase=True)\n",
        "    x = wide_residual_layer(x,n_filters[3],increase=True)\n",
        "    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D((8,8))(x)\n",
        "    x = Flatten()(x)\n",
        "    #x = Dense(classes_num,\n",
        "    #    activation='softmax',\n",
        "    #    kernel_initializer='he_normal',\n",
        "    #    kernel_regularizer=l2(WEIGHT_DECAY),\n",
        "    #    use_bias=False)(x)\n",
        "    return x\n",
        "\n",
        "def build_WideNet(depth = 28,k = 2) :\n",
        "  img_input = Input(shape=(32,32,3))\n",
        "  output = wide_residual_network(img_input,10,depth,k)\n",
        "  model = Model(img_input, output)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AlreadyExistsError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAlreadyExistsError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1361d40900c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Generic layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributed_training_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautocast_variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayer_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/mixed_precision/loss_scale_optimizer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_loss_scale_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFOptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madadelta\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madadelta_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madagrad\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madagrad_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madam\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0madam_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adadelta.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m keras_optimizers_gauge = tf.__internal__.monitoring.BoolGauge(\n\u001b[0;32m---> 37\u001b[0;31m     \"/tensorflow/api/keras/optimizers\", \"keras optimizer usage\", \"method\")\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m _DEFAULT_VALID_DTYPES = frozenset([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/monitoring.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, description, *labels)\u001b[0m\n\u001b[1;32m    359\u001b[0m     \"\"\"\n\u001b[1;32m    360\u001b[0m     super(BoolGauge, self).__init__('BoolGauge', _bool_gauge_methods,\n\u001b[0;32m--> 361\u001b[0;31m                                     len(labels), name, description, *labels)\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/monitoring.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, metric_name, metric_methods, label_length, *args)\u001b[0m\n\u001b[1;32m    133\u001b[0m           self._metric_name, len(self._metric_methods)))\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metric_methods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAlreadyExistsError\u001b[0m: Another metric with the same name already exists."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8F028jMNHl4"
      },
      "source": [
        "## Custom loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO6Du6Imhj8K"
      },
      "source": [
        "### Own contrastive loss\n",
        "Contrastive loss according to the formula given in the SimCLR paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziCcd02epuVA"
      },
      "source": [
        "@tf.function\n",
        "def contrastive_loss(proj,N, temperature = 0.5, additional_losses = False):\n",
        "  \"\"\"\n",
        "  NT-Xent (the normalized temperature-scaled cross entropy loss)\n",
        "  Contrastive loss according to the SimCLR scientific papper\n",
        "\n",
        "  Args : \n",
        "    proj : the outputs of the projection head\n",
        "    indexes : An index matrix needed to compute the loss, it is given by get_indexes but passed as an argument to minimize computation since it only depends of the shape of the features\n",
        "    temperature : temperature of the loss function\n",
        "                  it influences the performance of the learning but overall 0.5 seems to be a reliable value\n",
        "\n",
        "  Returns : \n",
        "    The contrastive loss \n",
        "  \"\"\"\n",
        "  states = tf.math.l2_normalize(proj, -1)\n",
        "  scores = tf.matmul(states, states, transpose_b=True)  # (bsz, bsz) #Matrice des s(i,j)\n",
        "  id = tf.eye(scores.shape[0])\n",
        "  scores = tf.math.subtract(scores,id)\n",
        "  num = tf.math.divide(scores,temperature)\n",
        "  indexes = tf.constant(get_indexes_l(int(N/2)))\n",
        "\n",
        "\n",
        "  l = tf.math.subtract( tf.math.reduce_logsumexp(num, axis = 1), num) # Matrice des l(i,j) \n",
        "  loss_dist_match = tf.reduce_mean(tf.gather_nd(l, indexes))\n",
        "  loss_dist_match = tf.scalar_mul(tf.constant(1/(2*N)),loss_dist_match)\n",
        "\n",
        "  if additional_losses :\n",
        "    loss_align = tf.scalar_mul(tf.constant(1/(4*N)),tf.reduce_mean((proj[::2] - proj[1::2])**2))\n",
        "    return loss_dist_match, loss_align\n",
        "  return loss_dist_match\n",
        "\n",
        "\n",
        "def get_indexes_l(n):\n",
        "  \"\"\" Indexes needed for the contrastive loss\n",
        "  \n",
        "  Args : \n",
        "    n : The batch size / 2\n",
        "\n",
        "  Returns :\n",
        "    The indexes needed for contrastive loss \n",
        "  \"\"\"\n",
        "  id1 = [[2*k+1,2*k] for k in range(n)]\n",
        "  id2 = [[2*k,2*k+1] for k in range(n)]\n",
        "  return id1 + id2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64QUAifGhsxR"
      },
      "source": [
        "### Gitgub Contrastive loss\n",
        "Contrastive loss given in the official github of the SimCLR paper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsl925Zrh1r9"
      },
      "source": [
        "def github_contrastive_loss(hidden,\n",
        "                         hidden_norm=True,\n",
        "                         temperature=1.0,\n",
        "                         tpu_context=None,\n",
        "                         weights=1.0):\n",
        "  \"\"\"Compute loss for model.\n",
        "  Args:\n",
        "    hidden: hidden vector (`Tensor`) of shape (2 * bsz, dim).\n",
        "    hidden_norm: whether or not to use normalization on the hidden vector.\n",
        "    temperature: a `floating` number for temperature scaling.\n",
        "    tpu_context: context information for tpu.\n",
        "    weights: a weighting number or vector.\n",
        "  Returns:\n",
        "    A loss scalar.\n",
        "    The logits for contrastive prediction task.\n",
        "    The labels for contrastive prediction task.\n",
        "  \"\"\"\n",
        "\n",
        "  LARGE_NUM = 1e9\n",
        "\n",
        "  # Get (normalized) hidden1 and hidden2.\n",
        "  if hidden_norm:\n",
        "    hidden = tf.math.l2_normalize(hidden, -1)\n",
        "  hidden1, hidden2 = tf.split(hidden, 2, 0)\n",
        "  batch_size = tf.shape(hidden1)[0]\n",
        "\n",
        "  # Gather hidden1/hidden2 across replicas and create local labels.\n",
        "  if tpu_context is not None:\n",
        "    hidden1_large = tpu_cross_replica_concat(hidden1, tpu_context)\n",
        "    hidden2_large = tpu_cross_replica_concat(hidden2, tpu_context)\n",
        "    enlarged_batch_size = tf.shape(hidden1_large)[0]\n",
        "    # TODO(iamtingchen): more elegant way to convert u32 to s32 for replica_id.\n",
        "    replica_id = tf.cast(tf.cast(xla.replica_id(), tf.uint32), tf.int32)\n",
        "    labels_idx = tf.range(batch_size) + replica_id * batch_size\n",
        "    labels = tf.one_hot(labels_idx, enlarged_batch_size * 2)\n",
        "    masks = tf.one_hot(labels_idx, enlarged_batch_size)\n",
        "  else:\n",
        "    hidden1_large = hidden1\n",
        "    hidden2_large = hidden2\n",
        "    labels = tf.one_hot(tf.range(batch_size), batch_size * 2)\n",
        "    masks = tf.one_hot(tf.range(batch_size), batch_size)\n",
        "\n",
        "  logits_aa = tf.matmul(hidden1, hidden1_large, transpose_b=True) / temperature\n",
        "  logits_aa = logits_aa - masks * LARGE_NUM\n",
        "  logits_bb = tf.matmul(hidden2, hidden2_large, transpose_b=True) / temperature\n",
        "  logits_bb = logits_bb - masks * LARGE_NUM\n",
        "  logits_ab = tf.matmul(hidden1, hidden2_large, transpose_b=True) / temperature\n",
        "  logits_ba = tf.matmul(hidden2, hidden1_large, transpose_b=True) / temperature\n",
        "\n",
        "  loss_a = tfc.losses.softmax_cross_entropy(\n",
        "      labels, tf.concat([logits_ab, logits_aa], 1), weights=weights)\n",
        "  loss_b = tfc.losses.softmax_cross_entropy(\n",
        "      labels, tf.concat([logits_ba, logits_bb], 1), weights=weights)\n",
        "  loss = loss_a + loss_b\n",
        "\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJfGJDUKS2WO"
      },
      "source": [
        "### Github loss function by margokhokhlova"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1WSLx5PTJP6"
      },
      "source": [
        "\n",
        "@tf.function\n",
        "def margokhokhlova_contrastive_loss(xi, xj,  tau=1, normalize=False):\n",
        "        ''' this loss is the modified torch implementation by M Diephuis here: https://github.com/mdiephuis/SimCLR/\n",
        "        the inputs:\n",
        "        xi, xj: image features extracted from a batch of images 2N, composed of N matching paints\n",
        "        tau: temperature parameter\n",
        "        normalize: normalize or not. seem to not be very useful, so better to try without.\n",
        "        '''\n",
        "\n",
        "        x = tf.keras.backend.concatenate((xi, xj), axis=0)\n",
        "\n",
        "        sim_mat = tf.keras.backend.dot(x, tf.keras.backend.transpose(x))\n",
        "\n",
        "        if normalize:\n",
        "            sim_mat_denom = tf.keras.backend.dot(tf.keras.backend.l2_normalize(x, axis=1).unsqueeze(1), tf.keras.backend.l2_normalize(x, axis=1).unsqueeze(1).T)\n",
        "            sim_mat = sim_mat / sim_mat_denom.clamp(min=1e-16)\n",
        "\n",
        "        sim_mat = tf.keras.backend.exp(sim_mat /tau)\n",
        "\n",
        "        x = xi * xj\n",
        "        if normalize:\n",
        "            sim_mat_denom = tf.keras.backend.l2_normalize(xi, dim=1) * tf.keras.backend.l2_normalize(xj, axis=1)\n",
        "            sim_match = tf.keras.backend.exp(tf.keras.backend.sum(x, axis=-1) / sim_mat_denom / tau)\n",
        "        else:\n",
        "            sim_match = tf.keras.backend.exp(tf.keras.backend.sum(x, axis=-1) / tau)\n",
        "\n",
        "        sim_match = tf.keras.backend.concatenate((sim_match, sim_match), axis=0)\n",
        "\n",
        "        norm_sum = tf.keras.backend.exp(tf.keras.backend.ones(tf.keras.backend.shape(x)[0]) / tau)\n",
        "\n",
        "        return tf.math.reduce_mean(-tf.keras.backend.log(sim_match / (tf.keras.backend.sum(sim_mat, axis=-1) - norm_sum)), name='contrastive_loss')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "les4y7T5NKaC"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENc28qmWSN50"
      },
      "source": [
        "In the algorithm given in the SimCLR paper, each image in augmented twice and the two augmentation are put next to each other in the batch. The order is important in the loss function and since I think this loss function is the most problematic part I wanted to keep the order given in the algorithm. \n",
        "\n",
        "An other problem that became quickly apparent is that the dataset I use has 32x32 images, which gives pretty blurry images. Even for a human recognizing what's on the image is not always easy and some kind of augmentation like cropping, blur or too much color change can easily make the job needlessly hard for the neural network.\n",
        "\n",
        "Lastly, I need to make the augmentation a @tf.function to reduce greatly the training time (from hours to minutes) which restricts me from doing some operations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVwmqqYE8fLn"
      },
      "source": [
        "### Own Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zn6tX--OAYO8"
      },
      "source": [
        "@tf.function\n",
        "def tensor_augment(x: tf.Tensor) -> tf.Tensor:\n",
        "    \"\"\"Color augmentation + random vertical flipt\n",
        "\n",
        "    Args:\n",
        "        x: Image\n",
        "\n",
        "    Returns:\n",
        "        Augmented image\n",
        "    \"\"\"\n",
        "    x = tf.image.random_hue(x, 0.08)\n",
        "    x = tf.image.random_saturation(x, 0.6, 1.6)\n",
        "    x = tf.image.random_brightness(x, 0.05)\n",
        "    x = tf.image.random_contrast(x, 0.7, 1.3)\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.clip_by_value(x,0,1)\n",
        "    return x\n",
        "\n",
        "@tf.function\n",
        "def double_tensor_augment(image) :\n",
        "    return tf.convert_to_tensor([tensor_augment(image),tensor_augment(image)])\n",
        "\n",
        "#@tf.function\n",
        "def augment_dataset(ds) :\n",
        "  ds = ds.map(double_tensor_augment)\n",
        "  out = ds.unbatch()\n",
        "  return out\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg8yp6gB8k84"
      },
      "source": [
        "### FixMatch Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOwOadrC8n3h"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy\n",
        "from scipy import ndimage\n",
        "from PIL import Image, ImageEnhance, ImageOps\n",
        "\n",
        "operations = {\n",
        "    'ShearX': lambda img, magnitude: shear_x(img, magnitude),\n",
        "    'ShearY': lambda img, magnitude: shear_y(img, magnitude),\n",
        "    'TranslateX': lambda img, magnitude: translate_x(img, magnitude),\n",
        "    'TranslateY': lambda img, magnitude: translate_y(img, magnitude),\n",
        "    'Rotate': lambda img, magnitude: rotate(img, magnitude),\n",
        "    'AutoContrast': lambda img, magnitude: auto_contrast(img, magnitude),\n",
        "    'Invert': lambda img, magnitude: invert(img, magnitude),\n",
        "    'Equalize': lambda img, magnitude: equalize(img, magnitude),\n",
        "    'Solarize': lambda img, magnitude: solarize(img, magnitude),\n",
        "    'Posterize': lambda img, magnitude: posterize(img, magnitude),\n",
        "    'Contrast': lambda img, magnitude: contrast(img, magnitude),\n",
        "    'Color': lambda img, magnitude: color(img, magnitude),\n",
        "    'Brightness': lambda img, magnitude: brightness(img, magnitude),\n",
        "    'Sharpness': lambda img, magnitude: sharpness(img, magnitude),\n",
        "    'Cutout': lambda img, magnitude: cutout(img, magnitude),\n",
        "}\n",
        "def apply_policy(img, policy):\n",
        "    if random.random() < policy[1]:\n",
        "        img = operations[policy[0]](img, policy[2])\n",
        "    if random.random() < policy[4]:\n",
        "        img = operations[policy[3]](img, policy[5])\n",
        "    # plt.imshow(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def transform_matrix_offset_center(matrix, x, y):\n",
        "    o_x = float(x) / 2 + 0.5\n",
        "    o_y = float(y) / 2 + 0.5\n",
        "    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
        "    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
        "    transform_matrix = offset_matrix @ matrix @ reset_matrix\n",
        "    return transform_matrix\n",
        "\n",
        "\n",
        "def shear_x(img, magnitude):\n",
        "    magnitudes = np.linspace(-0.3, 0.3, 11)\n",
        "\n",
        "    transform_matrix = np.array([[1, random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]), 0],\n",
        "                                 [0, 1, 0],\n",
        "                                 [0, 0, 1]])\n",
        "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
        "    affine_matrix = transform_matrix[:2, :2]\n",
        "    offset = transform_matrix[:2, 2]\n",
        "    img = np.stack([ndimage.interpolation.affine_transform(\n",
        "                    img[:, :, c],\n",
        "                    affine_matrix,\n",
        "                    offset) for c in range(img.shape[2])], axis=2)\n",
        "    return img\n",
        "\n",
        "\n",
        "def shear_y(img, magnitude):\n",
        "    magnitudes = np.linspace(-0.3, 0.3, 11)\n",
        "\n",
        "    transform_matrix = np.array([[1, 0, 0],\n",
        "                                 [random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]), 1, 0],\n",
        "                                 [0, 0, 1]])\n",
        "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
        "    affine_matrix = transform_matrix[:2, :2]\n",
        "    offset = transform_matrix[:2, 2]\n",
        "    img = np.stack([ndimage.interpolation.affine_transform(\n",
        "                    img[:, :, c],\n",
        "                    affine_matrix,\n",
        "                    offset) for c in range(img.shape[2])], axis=2)\n",
        "    return img\n",
        "\n",
        "\n",
        "def translate_x(img, magnitude):\n",
        "    magnitudes = np.linspace(-150/331, 150/331, 11)\n",
        "\n",
        "    transform_matrix = np.array([[1, 0, 0],\n",
        "                                 [0, 1, img.shape[1]*random.uniform(magnitudes[magnitude], magnitudes[magnitude+1])],\n",
        "                                 [0, 0, 1]])\n",
        "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
        "    affine_matrix = transform_matrix[:2, :2]\n",
        "    offset = transform_matrix[:2, 2]\n",
        "    img = np.stack([ndimage.interpolation.affine_transform(\n",
        "                    img[:, :, c],\n",
        "                    affine_matrix,\n",
        "                    offset) for c in range(img.shape[2])], axis=2)\n",
        "    return img\n",
        "\n",
        "\n",
        "def translate_y(img, magnitude):\n",
        "    magnitudes = np.linspace(-150/331, 150/331, 11)\n",
        "\n",
        "    transform_matrix = np.array([[1, 0, img.shape[0]*random.uniform(magnitudes[magnitude], magnitudes[magnitude+1])],\n",
        "                                 [0, 1, 0],\n",
        "                                 [0, 0, 1]])\n",
        "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
        "    affine_matrix = transform_matrix[:2, :2]\n",
        "    offset = transform_matrix[:2, 2]\n",
        "    img = np.stack([ndimage.interpolation.affine_transform(\n",
        "                    img[:, :, c],\n",
        "                    affine_matrix,\n",
        "                    offset) for c in range(img.shape[2])], axis=2)\n",
        "    return img\n",
        "\n",
        "\n",
        "def rotate(img, magnitude):\n",
        "    magnitudes = np.linspace(-30, 30, 11)\n",
        "\n",
        "    theta = np.deg2rad(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
        "    transform_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
        "                                 [np.sin(theta), np.cos(theta), 0],\n",
        "                                 [0, 0, 1]])\n",
        "    transform_matrix = transform_matrix_offset_center(transform_matrix, img.shape[0], img.shape[1])\n",
        "    affine_matrix = transform_matrix[:2, :2]\n",
        "    offset = transform_matrix[:2, 2]\n",
        "    img = np.stack([ndimage.interpolation.affine_transform(\n",
        "                    img[:, :, c],\n",
        "                    affine_matrix,\n",
        "                    offset) for c in range(img.shape[2])], axis=2)\n",
        "    return img\n",
        "\n",
        "\n",
        "def auto_contrast(img, magnitude):\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageOps.autocontrast(img)\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def invert(img, magnitude):\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageOps.invert(img)\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def equalize(img, magnitude):\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageOps.equalize(img)\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def solarize(img, magnitude):\n",
        "    magnitudes = np.linspace(0, 256, 11)\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageOps.solarize(img, random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def posterize(img, magnitude):\n",
        "    magnitudes = np.linspace(4, 8, 11)\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageOps.posterize(img, int(round(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def contrast(img, magnitude):\n",
        "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageEnhance.Contrast(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def color(img, magnitude):\n",
        "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageEnhance.Color(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def brightness(img, magnitude):\n",
        "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageEnhance.Brightness(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def sharpness(img, magnitude):\n",
        "    magnitudes = np.linspace(0.1, 1.9, 11)\n",
        "\n",
        "    img = (img * 255).astype(np.uint8)\n",
        "    img = Image.fromarray(img)\n",
        "    img = ImageEnhance.Sharpness(img).enhance(random.uniform(magnitudes[magnitude], magnitudes[magnitude+1]))\n",
        "    img = np.array(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def cutout(org_img, magnitude=None):\n",
        "    magnitudes = np.linspace(0, 60/331, 11)\n",
        "\n",
        "    img = np.copy(org_img)\n",
        "    mask_val = img.mean()\n",
        "\n",
        "    if magnitude is None:\n",
        "        mask_size = 16\n",
        "    else:\n",
        "        mask_size = int(round(img.shape[0]*random.uniform(magnitudes[magnitude], magnitudes[magnitude+1])))\n",
        "    top = np.random.randint(0 - mask_size//2, img.shape[0] - mask_size)\n",
        "    left = np.random.randint(0 - mask_size//2, img.shape[1] - mask_size)\n",
        "    bottom = top + mask_size\n",
        "    right = left + mask_size\n",
        "\n",
        "    if top < 0:\n",
        "        top = 0\n",
        "    if left < 0:\n",
        "        left = 0\n",
        "\n",
        "    img[top:bottom, left:right, :].fill(mask_val)\n",
        "\n",
        "    return img\n",
        "\n",
        "policies = [\n",
        "              ['Invert', 0.1, 7, 'Contrast', 0.2, 6],\n",
        "              ['Rotate', 0.7, 2, 'TranslateX', 0.3, 9],\n",
        "              ['Sharpness', 0.8, 1, 'Sharpness', 0.9, 3],\n",
        "              ['ShearY', 0.5, 8, 'TranslateY', 0.7, 9],\n",
        "              ['AutoContrast', 0.5, 8, 'Equalize', 0.9, 2],\n",
        "              ['ShearY', 0.2, 7, 'Posterize', 0.3, 7],\n",
        "              ['Color', 0.4, 3, 'Brightness', 0.6, 7],\n",
        "              ['Sharpness', 0.3, 9, 'Brightness', 0.7, 9],\n",
        "              ['Equalize', 0.6, 5, 'Equalize', 0.5, 1],\n",
        "              ['Contrast', 0.6, 7, 'Sharpness', 0.6, 5],\n",
        "              ['Color', 0.7, 7, 'TranslateX', 0.5, 8],\n",
        "              ['Equalize', 0.3, 7, 'AutoContrast', 0.4, 8],\n",
        "              ['TranslateY', 0.4, 3, 'Sharpness', 0.2, 6],\n",
        "              ['Brightness', 0.9, 6, 'Color', 0.2, 8],\n",
        "              ['Solarize', 0.5, 2, 'Invert', 0, 0.3],\n",
        "              ['Equalize', 0.2, 0, 'AutoContrast', 0.6, 0],\n",
        "              ['Equalize', 0.2, 8, 'Equalize', 0.6, 4],\n",
        "              ['Color', 0.9, 9, 'Equalize', 0.6, 6],\n",
        "              ['AutoContrast', 0.8, 4, 'Solarize', 0.2, 8],\n",
        "              ['Brightness', 0.1, 3, 'Color', 0.7, 0],\n",
        "              ['Solarize', 0.4, 5, 'AutoContrast', 0.9, 3],\n",
        "              ['TranslateY', 0.9, 9, 'TranslateY', 0.7, 9],\n",
        "              ['AutoContrast', 0.9, 2, 'Solarize', 0.8, 3],\n",
        "              ['Equalize', 0.8, 8, 'Invert', 0.1, 3],\n",
        "              ['TranslateY', 0.7, 9, 'AutoContrast', 0.9, 1],\n",
        "          ]\n",
        "def augment(image):\n",
        "  augmented_image = apply_policy(image, policies[random.randrange(len(policies))])\n",
        "  #augmented_image = cutout(augmented_image)\n",
        "  \n",
        "  return augmented_image\n",
        "\n",
        "\n",
        "def augment_batch_fixmatch(batch) :\n",
        "  l = [ x for x in map(lambda x : [augment(image = np.float32(x)),augment(image = np.float32(x))] , batch)]\n",
        "  out = []\n",
        "  for x in l :\n",
        "    out += x\n",
        "  return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k030flOYNNz4"
      },
      "source": [
        "##Custom model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn5y2GCDNhU_"
      },
      "source": [
        "### Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcJY6mhFvQGa"
      },
      "source": [
        "class CustomModel(keras.Model):\n",
        "  def __init__(self, body, head, Fixmatch = False, model_name = \"\"):\n",
        "    super(CustomModel, self).__init__()\n",
        "    self.body = body\n",
        "    self.head = head\n",
        "    self.Fixmatch = Fixmatch\n",
        "    self.optimizer = -1\n",
        "    now = datetime.now()\n",
        "    init_time = now.astimezone(tz).strftime(\" %H:%M\")\n",
        "    self.model_name = model_name + init_time\n",
        "\n",
        "\n",
        "  def unfreeze_body_weights(self) :\n",
        "    for layers in self.body.layers : # unfreeze weights\n",
        "      layers.trainable= True\n",
        "\n",
        "  def freeze_body_weights(self):\n",
        "    for layers in self.body.layers : # unfreeze weights\n",
        "      layers.trainable= False\n",
        "\n",
        "  @tf.function\n",
        "  def pretrain_step(self,batch_x, tot_loss, tot_loss_align = 0, additional_losses = False) :\n",
        "    with tf.GradientTape() as tape: # GradientTape to record the operations and compute the gradiants\n",
        "      y_pred = self(batch_x, training = True)\n",
        "      N = len(y_pred)\n",
        "      if additional_losses :\n",
        "        loss_value, loss_align = contrastive_loss(y_pred,N,additional_losses = additional_losses)\n",
        "        tot_loss_align += loss_align\n",
        "      else :\n",
        "        loss_value = contrastive_loss(y_pred,N, additional_losses = additional_losses)\n",
        "        #loss_value = github_contrastive_loss(y_pred)\n",
        "      tot_loss += loss_value\n",
        "      loss_value += sum(self.losses)\n",
        "    grads = tape.gradient(loss_value, self.trainable_weights) # computes the gradients\n",
        "    self.optimizer.apply_gradients(zip(grads, self.trainable_weights))# Using the gradiants to optimize weights\n",
        "    return tot_loss,tot_loss_align, grads, y_pred\n",
        "\n",
        "\n",
        "  def custom_pretraining(self,dataset, epochs = 1,additional_losses = False, regular_save = False, \n",
        "                         verbose = 0, batch_size = 16, normalization = None):\n",
        "\n",
        "    metric_contrastive_loss = []\n",
        "    metric_loss_align = []\n",
        "\n",
        "    if verbose == 1 :\n",
        "      iterator1 = tqdm(range(1,epochs+1), desc=\"Contrasive training\", unit = \"epoch\", position=0, leave=True)\n",
        "    else :\n",
        "      iterator1 = range(1,epochs+1)\n",
        "\n",
        "    for epoch in iterator1 : # for each epoch\n",
        "      shuffled_dataset = dataset.shuffle(1000)\n",
        "      augmented_dataset = augment_dataset(shuffled_dataset)\n",
        "      augmented_dataset = augmented_dataset.batch(batch_size*2)\n",
        "      if normalization == \"cifar10\":\n",
        "        augmented_dataset = augmented_dataset.map(normalize_cifar10_element)\n",
        "      tot_loss = 0\n",
        "      tot_loss_align = 0\n",
        "\n",
        "      if verbose == 2 :\n",
        "        iterator2 = enumerate(tqdm(augmented_dataset, desc=f\"Epoch {epoch}\", unit=\"batch\",position=0, leave=True ))\n",
        "      else : \n",
        "        iterator2 = enumerate(augmented_dataset) \n",
        "\n",
        "      for step, batch_x in iterator2  : # for each batch\n",
        "        tot_loss,tot_loss_align, grads, y_pred = self.pretrain_step(batch_x,tot_loss, tot_loss_align, additional_losses)\n",
        "      \n",
        "      metric_contrastive_loss.append(tot_loss)\n",
        "      metric_loss_align.append(tot_loss_align)\n",
        "\n",
        "      if verbose == 2 :\n",
        "        if additional_losses :\n",
        "          print(f'Loss : {tot_loss} --- Loss Align : {tot_loss_align}')\n",
        "        else :\n",
        "          print(f'Loss : {tot_loss}')\n",
        "\n",
        "      if regular_save & ((epoch)%10 == 0):\n",
        "        tf.keras.models.save_model(self, PATH + str(f'{self.model_name} Pre-training -> epoch : {epoch}, optimizer : {self.optimizer}, FixMatch : {self.Fixmatch}'), overwrite=True)\n",
        "\n",
        "    if additional_losses :\n",
        "      return metric_contrastive_loss, metric_loss_align\n",
        "    else :\n",
        "      return metric_contrastive_loss\n",
        "\n",
        "  def call(self,input_tensor, training = False):\n",
        "    x = self.body(input_tensor, training = training)\n",
        "    x = self.head(x, training = training)\n",
        "    return x\n",
        "\n",
        "  def predict_classes(self,input):\n",
        "    return np.argmax(self.predict(input),axis = 1)\n",
        "\n",
        "  def compile(self, optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, **kwargs):\n",
        "    self.optimizer = optimizer\n",
        "    return super().compile(optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, steps_per_execution, **kwargs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4pOr0XeNoEc"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBNYPClR7mRo"
      },
      "source": [
        "### Save & plot data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L74F3rcQHZpv"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZuIrs-IJGFs"
      },
      "source": [
        "\n",
        "def list_to_xlsx(x,y,doc, l, vertical = True, format = None) :\n",
        "  length = 0\n",
        "  if vertical :\n",
        "    for i,data in enumerate(l) :\n",
        "      doc.write(x+i,y, data, format)\n",
        "      length = i\n",
        "    return xl_rowcol_to_cell(x,y)+\":\"+xl_rowcol_to_cell(x+length,y)\n",
        "  else :\n",
        "    for i,data in enumerate(l) :\n",
        "      doc.write(x,y+i, data, format)\n",
        "    return xl_rowcol_to_cell(x,y)+\":\"+xl_rowcol_to_cell(x,y+length)\n",
        "\n",
        "def xlsx_min(inter):\n",
        "  return \"=MIN(\"+inter+\")\"\n",
        "def xlsx_max(inter):\n",
        "  return \"=MAX(\"+inter+\")\"\n",
        "\n",
        "def xlsx_format_data(x,y,wb, ws,l, name, vertical = True, target = None) :\n",
        "    min_max_format = wb.add_format({'bold': True, 'font_color': 'red'})\n",
        "    target_format = wb.add_format({'bold': True, 'font_color': 'red', 'bg_color':'lime'})\n",
        "    title_format = wb.add_format({'bold': True})\n",
        "    if vertical :\n",
        "      ws.write(x,y,name,title_format)\n",
        "      inter_cl = list_to_xlsx(x+5,y,ws,l,vertical)\n",
        "      if target == \"max\":\n",
        "        ws.write_formula(x+3,y,xlsx_min(inter_cl),min_max_format)\n",
        "        ws.write_formula(x+2,y,xlsx_max(inter_cl),target_format)\n",
        "      elif target == \"min\":\n",
        "        ws.write_formula(x+3,y,xlsx_min(inter_cl),target_format)\n",
        "        ws.write_formula(x+2,y,xlsx_max(inter_cl),min_max_format)\n",
        "      else :\n",
        "        ws.write_formula(x+3,y,xlsx_min(inter_cl),min_max_format)\n",
        "        ws.write_formula(x+2,y,xlsx_max(inter_cl),min_max_format)\n",
        "    else :\n",
        "      ws.write(x,y,name,title_format)\n",
        "      inter_cl = list_to_xlsx(x,y+5,ws,l,vertical)\n",
        "      if target == \"max\" :\n",
        "        ws.write_formula(x,y+3,xlsx_min(inter_cl),min_max_format)\n",
        "        ws.write_formula(x,y+2,xlsx_max(inter_cl),target_format)\n",
        "      elif target == \"min\" :\n",
        "        ws.write_formula(x,y+3,xlsx_min(inter_cl),target_format)\n",
        "        ws.write_formula(x,y+2,xlsx_max(inter_cl),min_max_format)\n",
        "      else :\n",
        "        ws.write_formula(x,y+3,xlsx_min(inter_cl),min_max_format)\n",
        "        ws.write_formula(x,y+2,xlsx_max(inter_cl),min_max_format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEivDAXaHk6q"
      },
      "source": [
        "#### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHCzoGJ-Wkti"
      },
      "source": [
        "def stories(story_metrics_cl, story_metrics_la, story_fit, story_ft, story_eval, step) :\n",
        "  for i in range(len(story_metrics_cl)) :\n",
        "    nb_epochs = step * i\n",
        "    metrics_cl = story_metrics_cl[i]\n",
        "    metrics_la = story_metrics_la[i]\n",
        "    history_fit = story_fit[i]\n",
        "    history_fine_tune = story_ft[i]\n",
        "    evaluation = story_eval[i]\n",
        "\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.title(\"Losses over contrastive training\")\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(metrics_cl, label = \"Contrastive Loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(metrics_la, label = \"Loss Align\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "\n",
        "    plt.savefig(PATH+f\"epochs{nb_epochs}_contrastive_training.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.title(\"Losses & Accuracies over fitting\")\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(history_fit.history['accuracy'], label = \"training accuracy\")\n",
        "    plt.plot(history_fit.history['val_accuracy'], label = \"validation accuracy\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(history_fit.history['loss'], label = \"training loss\")\n",
        "    plt.plot(history_fit.history['val_loss'], label = \"validation loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "\n",
        "    plt.savefig(PATH+f\"_epochs{nb_epochs}_fitting.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.title(\"Losses & Accuracies over fine tuning\")\n",
        "\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.plot(history_fine_tune.history['accuracy'], label = \"training accuracy\")\n",
        "    plt.plot(history_fine_tune.history['val_accuracy'], label = \"validation accuracy\")\n",
        "    plt.plot([evaluation[1] for  _ in range(len(history_fine_tune.history['val_accuracy']))], label = \"test accuracy\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"accuracy\")\n",
        "\n",
        "    plt.subplot(2,1,2)\n",
        "    plt.plot(history_fine_tune.history['loss'], label = \"training loss\")\n",
        "    plt.plot(history_fine_tune.history['val_loss'], label = \"validation loss\")\n",
        "    plt.plot([evaluation[0] for  _ in range(len(history_fine_tune.history['val_loss']))], label =\"test loss\")\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.xlabel(\"epoch\")\n",
        "    plt.ylabel(\"loss\")\n",
        "\n",
        "    plt.savefig(PATH+f\"_epochs{nb_epochs}_fine_tuning.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def save_stories_xlsx(story_metrics_cl, story_metrics_la, story_fit, story_ft, story_eval, step, name) :\n",
        "  \n",
        "  writer = pd.ExcelWriter(name+'.xlsx', engine='xlsxwriter',options={'nan_inf_to_errors': True})\n",
        "  workbook = writer.book\n",
        "  target_format = workbook.add_format({'bold': True, 'font_color': 'red'})\n",
        "  title_format = workbook.add_format({'bold': True})\n",
        "\n",
        "  for i in range(len(story_metrics_cl)) :\n",
        "    nb_epochs = step * (i+1)\n",
        "    metrics_cl = story_metrics_cl[i]\n",
        "    metrics_la = story_metrics_la[i]\n",
        "    history_fit = story_fit[i]\n",
        "    history_fine_tune = story_ft[i]\n",
        "    evaluation = story_eval[i]\n",
        "    worksheet = workbook.add_worksheet(f\"With {nb_epochs} epochs\")\n",
        "    #worksheet.write(0,1, \"Target value\",title_format)\n",
        "    worksheet.write(4,0, \"min\",title_format)\n",
        "    worksheet.write(5,0, \"max\",title_format)\n",
        "    \n",
        "    worksheet.set_column(2, 2, 15)\n",
        "    worksheet.set_column(4, 5, 15)\n",
        "    worksheet.set_column(7, 10, 15)\n",
        "    worksheet.set_column(12, 15, 15)\n",
        "\n",
        "    xlsx_format_data(2,4,workbook,worksheet, metrics_cl, \"Contrastive loss\", target = \"min\")\n",
        "    xlsx_format_data(2,5,workbook,worksheet, metrics_la, \"Loss align\", target = \"min\")\n",
        "\n",
        "    xlsx_format_data(2,7,workbook,worksheet, history_fit.history[\"loss\"], \"Fit - Loss\", target = \"min\")\n",
        "    xlsx_format_data(2,8,workbook,worksheet, history_fit.history[\"val_loss\"], \"Fit - Validation loss\", target = \"min\")\n",
        "    xlsx_format_data(2,9,workbook,worksheet, history_fit.history[\"accuracy\"], \"Fit - Accuracy\", target = \"max\")\n",
        "    xlsx_format_data(2,10,workbook,worksheet, history_fit.history[\"val_accuracy\"], \"Fit - Validation accuracy\", target = \"max\")\n",
        "\n",
        "    xlsx_format_data(2,12,workbook,worksheet, history_fine_tune.history[\"loss\"], \"Fine tune - Loss\", target = \"min\")\n",
        "    xlsx_format_data(2,13,workbook,worksheet, history_fine_tune.history[\"val_loss\"], \"Fine tune - Validation loss\", target = \"min\")\n",
        "    xlsx_format_data(2,14,workbook,worksheet, history_fine_tune.history[\"accuracy\"], \"Fine tune - Accuracy\", target = \"max\")\n",
        "    xlsx_format_data(2,15,workbook,worksheet, history_fine_tune.history[\"val_accuracy\"], \"Fine tune - Validation accuracy\", target = \"max\")\n",
        "\n",
        "    xlsx_format_data(2,2,workbook,worksheet, evaluation, \"Test Loss & Accuracy\")\n",
        "\n",
        "  writer.save()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks8Bdlt9dfBr"
      },
      "source": [
        "## General evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGnVXf0Qdjqy"
      },
      "source": [
        "Training_loop_dic = {\n",
        "    contrastive_training_steps : 20,\n",
        "    regular_save : 2, # None / must divide contrastive_training_steps\n",
        "    training_steps : 10,\n",
        "    fine_tuning_steps : 10,\n",
        "    body : 'resnet', # resnet / widenet / conv\n",
        "    cb_patience : 5,\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "now = datetime.now()\n",
        "day = now.astimezone(tz).strftime(\"%d-%m-%Y\")\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début de l\\'import et du traitement des données : {date}')\n",
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début du protocole : {date}')\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "\n",
        "story_metrics_cl = []\n",
        "story_metrics_la = []\n",
        "story_fit = []\n",
        "story_ft = []\n",
        "story_eval = []\n",
        "\n",
        "for i in range(Training_loop_dic[\"contrastive_training_steps\"]) :\n",
        "  if Training_loop_dic[\"regular_save\"] == None :\n",
        "\n",
        "  else :\n",
        "    nb_epochs = Training_loop_dic[\"regular_save\"]*(i+1)\n",
        "    now = datetime.now()\n",
        "    date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "    print(f'Début du test avec {nb_epochs} epochs : {date}')\n",
        "\n",
        "  classifier = build_classifier()\n",
        "  projection_head = build_projection_head()\n",
        "  if i == 0 :\n",
        "    if Training_loop_dic[\"body\"] == \"widenet\" :\n",
        "      body = build_WideNet()\n",
        "    elif Training_loop_dic[\"body\"] == \"resnet\" :\n",
        "      body = build_resnet()\n",
        "    elif Training_loop_dic[\"body\"] == \"conv\" :\n",
        "      body = build_simple_conv()\n",
        "      \n",
        "    model = CustomModel(body,projection_head)\n",
        "    model.build([None,32,32,3])\n",
        "    model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  else :\n",
        "    model.load_weights(\"temp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17H2xO8NOK7u"
      },
      "source": [
        "## Teste sur le nombre d'épochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zz_W2f5n7fC6"
      },
      "source": [
        "### With shallow NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQVm1rP4OFvU"
      },
      "source": [
        "now = datetime.now()\n",
        "day = now.astimezone(tz).strftime(\"%d-%m-%Y\")\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début de l\\'import et du traitement des données : {date}')\n",
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début du protocole : {date}')\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "\n",
        "story_metrics_cl = []\n",
        "story_metrics_la = []\n",
        "story_fit = []\n",
        "story_ft = []\n",
        "story_eval = []\n",
        "\n",
        "for i in range(10) :\n",
        "  nb_epochs = 2*(i+1)\n",
        "  now = datetime.now()\n",
        "  date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "  print(f'Début du test avec {nb_epochs} epochs : {date}')\n",
        "\n",
        "  classifier = build_classifier()\n",
        "  projection_head = build_projection_head()\n",
        "\n",
        "  if i == 0 :\n",
        "    body = build_simple_conv()\n",
        "    model = CustomModel(body,projection_head)\n",
        "    model.build([None,32,32,3])\n",
        "    model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  else :\n",
        "    model.load_weights(\"temp\")\n",
        "\n",
        "\n",
        "\n",
        "  model.unfreeze_body_weights()\n",
        "  metrics_cl = model.custom_pretraining(unlabeled_ds, epochs=2, batch_size = 64, additional_losses = True, verbose = 2, normalization=\"cifar10\")\n",
        "  story_metrics_cl.append(metrics_cl)\n",
        "  metrics_la = []\n",
        "  story_metrics_la.append(metrics_la)\n",
        "\n",
        "  model.save_weights(\"temp\")\n",
        "\n",
        "  model2 = CustomModel(body, classifier)\n",
        "  model2.build([None,32,32,3])\n",
        "  model2.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  model2.freeze_body_weights()\n",
        "  history_fit = model2.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 100, callbacks=[callback], verbose = 1)  \n",
        "  story_fit.append(history_fit)\n",
        "\n",
        "  model3 = CustomModel(body, classifier)\n",
        "  model3.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  #model3.unfreeze_body_weights()\n",
        "  history_fine_tune = model3.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 5, verbose = 1)  \n",
        "  story_ft.append(history_fine_tune)\n",
        "\n",
        "  evaluation = model3.evaluate(test_ds,batch_size=128)\n",
        "  story_eval.append(evaluation)\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Fin du protocole : {date}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJEZcfF7uLOO"
      },
      "source": [
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "body = build_simple_conv()\n",
        "model_test = CustomModel(body, build_classifier())\n",
        "model_test.build([(None, 32,32, 3)])\n",
        "model_test.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model_test.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 100, callbacks=[callback], verbose = 2)\n",
        "evaluation = model_test.evaluate(test_ds,batch_size=128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDjXE52xtP34"
      },
      "source": [
        "save_stories_xlsx(story_metrics_cl, story_metrics_la, story_fit, story_ft, story_eval, 2, PATH+\"16th test - shallow nn - new loss, github loss\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRCLWvCV9LHN"
      },
      "source": [
        "### With WideNet with saves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQejwyck9NQO"
      },
      "source": [
        "now = datetime.now()\n",
        "day = now.astimezone(tz).strftime(\"%d-%m-%Y\")\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début de l\\'import et du traitement des données : {date}')\n",
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début du protocole : {date}')\n",
        "\n",
        ")\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "\n",
        "story_metrics_cl = []\n",
        "story_metrics_la = []\n",
        "story_fit = []\n",
        "story_ft = []\n",
        "story_eval = []\n",
        "\n",
        "for i in range(10) :\n",
        "  nb_epochs = 2*(i+1)\n",
        "  now = datetime.now()\n",
        "  date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "  print(f'Début du test avec {nb_epochs} epochs : {date}')\n",
        "\n",
        "  classifier = build_classifier()\n",
        "  projection_head = build_projection_head()\n",
        "\n",
        "  if i == 0 :\n",
        "    body = build_WideNet()\n",
        "    model = CustomModel(body,projection_head)\n",
        "    model.build([None,32,32,3])\n",
        "    model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  else :\n",
        "    model.load_weights(\"temp\")\n",
        "\n",
        "\n",
        "\n",
        "  model.unfreeze_body_weights()\n",
        "  metrics_cl = model.custom_pretraining(unlabeled_ds, epochs=2, batch_size = 64, additional_losses = True, verbose = 2, normalization=\"cifar10\")\n",
        "  story_metrics_cl.append(metrics_cl)\n",
        "  metrics_la = []\n",
        "  story_metrics_la.append(metrics_la)\n",
        "\n",
        "  model.save_weights(\"temp\")\n",
        "\n",
        "  model2 = CustomModel(body, classifier)\n",
        "  model2.build([None,32,32,3])\n",
        "  model2.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  model2.freeze_body_weights()\n",
        "  history_fit = model2.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 100, callbacks=[callback], verbose = 1)  \n",
        "  story_fit.append(history_fit)\n",
        "\n",
        "  model3 = CustomModel(body, classifier)\n",
        "  model3.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  #model3.unfreeze_body_weights()\n",
        "  history_fine_tune = model3.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 5, verbose = 1)  \n",
        "  story_ft.append(history_fine_tune)\n",
        "\n",
        "  evaluation = model3.evaluate(test_ds,batch_size=128)\n",
        "  story_eval.append(evaluation)\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Fin du protocole : {date}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSuvIfvtnt31"
      },
      "source": [
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "body = build_WideNet(depth = 28,k = 2)\n",
        "model_test = CustomModel(body, build_classifier())\n",
        "model_test.build([(None, 32,32, 3)])\n",
        "model_test.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "history = model_test.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 100, callbacks=[callback], verbose = 2)\n",
        "evaluation = model_test.evaluate(test_ds,batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWyXIpBMn6JK"
      },
      "source": [
        "save_stories_xlsx(story_metrics_cl, story_metrics_la, story_fit, story_ft, story_eval, 2, \"6th test - WideNet nn with better normalization with regular save\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muFsIeJIFvkG"
      },
      "source": [
        "## With trained ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHfSMwavFr-J"
      },
      "source": [
        "now = datetime.now()\n",
        "day = now.astimezone(tz).strftime(\"%d-%m-%Y\")\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début de l\\'import et du traitement des données : {date}')\n",
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Début du protocole : {date}')\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "\n",
        "story_metrics_cl = []\n",
        "story_metrics_la = []\n",
        "story_fit = []\n",
        "story_ft = []\n",
        "story_eval = []\n",
        "\n",
        "for i in range(10) :\n",
        "  nb_epochs = 2*(i+1)\n",
        "  now = datetime.now()\n",
        "  date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "  print(f'Début du test avec {nb_epochs} epochs : {date}')\n",
        "\n",
        "  classifier = build_classifier()\n",
        "  projection_head = build_projection_head()\n",
        "\n",
        "  if i == 0 :\n",
        "    body = build_resnet()\n",
        "    model = CustomModel(body,projection_head)\n",
        "    model.build([None,32,32,3])\n",
        "    model.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  else :\n",
        "    model.load_weights(\"temp\")\n",
        "\n",
        "\n",
        "\n",
        "  model.unfreeze_body_weights()\n",
        "  metrics_cl, metrics_la = model.custom_pretraining(unlabeled_ds, epochs=2, batch_size = 64, additional_losses = True, verbose = 2, normalization=\"cifar10\")\n",
        "  story_metrics_cl.append(metrics_cl)\n",
        "  story_metrics_la.append(metrics_la)\n",
        "\n",
        "  model.save_weights(\"temp\")\n",
        "\n",
        "  model2 = CustomModel(body, classifier)\n",
        "  model2.build([None,32,32,3])\n",
        "  model2.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  model2.freeze_body_weights()\n",
        "  history_fit = model2.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 5, callbacks=[callback], verbose = 1)  \n",
        "  story_fit.append(history_fit)\n",
        "\n",
        "  model3 = CustomModel(body, classifier)\n",
        "  model3.build([None,32,32,3])\n",
        "  model3.compile(optimizer= keras.optimizers.Adam(learning_rate=1e-5),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "  \n",
        "  model3.unfreeze_body_weights()\n",
        "  history_fine_tune = model3.fit(train_ds, validation_data = val_ds, batch_size=32, epochs = 5, verbose = 1)  \n",
        "  story_ft.append(history_fine_tune)\n",
        "\n",
        "  evaluation = model3.evaluate(test_ds,batch_size=128)\n",
        "  story_eval.append(evaluation)\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.astimezone(tz).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
        "print(f'Fin du protocole : {date}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lkc9WnUAF51M"
      },
      "source": [
        "save_stories_xlsx(story_metrics_cl, story_metrics_la, story_fit, story_ft, story_eval, 2,name = \"2nd test - WideNet nn with normalized images without regular save, without body freeze on fine tune\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW7JiiOQDyof"
      },
      "source": [
        "# Tests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwYJH4PbbfVI"
      },
      "source": [
        "## Unit tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrDa6sWOwkms"
      },
      "source": [
        "### Testes tf.data.dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Faa2vji5Oym"
      },
      "source": [
        "#### Few tests ot show how tfds.load works with split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRQYpwbI9a_Y"
      },
      "source": [
        "#train_ds, test_ds = tfds.load(\"cifar10\", split=['train','test'], as_supervised=True, batch_size=10)\n",
        "#print(len(train_ds))\n",
        "#print(len(test_ds))\n",
        "\n",
        "#unlabeled_ds = tfds.load(\"cifar10\", split='train', as_supervised=True, batch_size=10)\n",
        "#labeled_ds = tfds.load(\"cifar10\", split='test',as_supervised=True, batch_size=10)\n",
        "#print(len(unlabeled_ds))\n",
        "#print(len(labeled_ds))\n",
        "\n",
        "#train_ds, test_ds, validation_ds = tfds.load(\"cifar10\", split=['test[0%:80%]','test[80%:90%]','test[90%:100%]'],as_supervised=True, batch_size=10)\n",
        "#print(len(train_ds))\n",
        "#print(len(test_ds))\n",
        "#print(len(validation_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrqoDMFY54u7"
      },
      "source": [
        "#### Few tests to show the structure of a dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGqKhcIHwoGy"
      },
      "source": [
        "train_ds, test_ds = tfds.load(\"cifar10\", split=['train','test'], as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8x5KdHZnT6G"
      },
      "source": [
        "train_ds.shuffle(buffer_size=100)\n",
        "train_ds = train_ds.take(1)\n",
        "x = train_ds.map(get_x)\n",
        "for image in x :\n",
        "  # We go over all the batch but there is only one thanks to take()\n",
        "  print(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd5csFe61yds"
      },
      "source": [
        "xtds = train_ds.map(get_x)\n",
        "#print(xtds)\n",
        "#print(ytds)\n",
        "xtds = xtds.map(format_batch)\n",
        "ytds = train_ds.map(get_y)\n",
        "\n",
        "#print(ytds)\n",
        "#plot_batch(ytds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlGc2mE86l0F"
      },
      "source": [
        "#### Test of the dataset augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tnpbPHwyZCm"
      },
      "source": [
        "def extract_batch(ds) :\n",
        "  for batch in ds :\n",
        "    return batch\n",
        "    \n",
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "shuffled_dataset = unlabeled_ds.shuffle(1000)\n",
        "augmented_dataset = augment_dataset(shuffled_dataset)\n",
        "augmented_dataset = augmented_dataset.batch(10)\n",
        "x = augmented_dataset.take(1)\n",
        "print(x)\n",
        "x = extract_batch(x)\n",
        "plot_batch(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbmMwYjFuW2x"
      },
      "source": [
        "###Tests of contrastive loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvxYXroZdMqk"
      },
      "source": [
        "The contrastive loss is, I think, the main difficulty of this project and I probably still to this day didn't make one that works as I want to and is the reason why the performances I get aren't so great.\n",
        "\n",
        "Difficulty 1 : The loss function is quite a complicated math function and test the loss function by hand in a unit test is not really easy nor explicit.\n",
        "\n",
        "Difficulty 2 : The loss function needs to be a @tf.function to drasticaly cut down the training time. This mean I can't do any operation in the loss function but I am limited to a small set of operations (ex: the module tf.math) to reproduce the formula and making it harder to understand what other implementations do.\n",
        "\n",
        "Difficulty 3 : I found several loss function that claim to be implementation of the SimCLR paper (including the one given on the github repository of the SimCLR paper writers) but none of them seem to match the mathematical formula given in the paper. Probably either because of a mistake or a different implementation somewhere else (of the datasets for instance) changing the loss function.\n",
        "\n",
        "Test 1 : I tested the variations of the loss functions on small test cases, it seems that my function has the right variations.\n",
        "\n",
        "Test 2 : I tested the corelation between some of the loss funciton I found and my own and I didn't find any corelation between them so at most one of them is correct\n",
        "\n",
        "Conclusion : I still don't know if my custom loss function is correct but it is the best i have for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fk8viVLZuY_9"
      },
      "source": [
        "\n",
        "proj_test1 = tf.constant([[0,0,0,0.9],[0,0,0,0.88],[0.5,0.5,0,0],[0.89,0.1,0,0]])\n",
        "proj_test2 = tf.constant([[0,0,0,0.9],[0,0,0,0.88],[0.7,0.3,0,0],[0.89,0.1,0,0]])\n",
        "print(contrastive_loss(proj_test1,indices = get_indices_l(2)))\n",
        "print(contrastive_loss(proj_test2,indices = get_indices_l(2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zmI5ITIjAAY"
      },
      "source": [
        "\n",
        "unlabeled_ds, train_ds, test_ds, val_ds = generate_datasets()\n",
        "l1 = []\n",
        "l2 = []\n",
        "for i in trange(200) :\n",
        "  shuffled_dataset = unlabeled_ds.shuffle(50000)\n",
        "  augmented_dataset = augment_dataset(shuffled_dataset)\n",
        "  augmented_dataset = augmented_dataset.batch(10)\n",
        "  x = augmented_dataset.take(1)\n",
        "\n",
        "  body = build_simple_conv()\n",
        "  head = build_projection_head()\n",
        "  model = CustomModel(body,head)\n",
        "  model.build([None,32,32,3]) \n",
        "  model.compile(optimizer= keras.optimizers.Adam(),\n",
        "                  loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  for batch in x :\n",
        "    y = model(batch)\n",
        "    #print(y)  #  Just one\n",
        "\n",
        "  l1.append(float(contrastive_loss(y, 10)))\n",
        "  l2.append(float(github_contrastive_loss(y)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TWLPPOfjuOr"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(l1,l2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggfSXSwd03Fl"
      },
      "source": [
        "### Test of freeze & unfreeze"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqzDFmVN2re1"
      },
      "source": [
        "def count_params(model):\n",
        "  nb_non_trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.non_trainable_weights])\n",
        "  nb_trainable_params = np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_weights])\n",
        "  return nb_non_trainable_params, nb_trainable_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hJ4SPNM07xk"
      },
      "source": [
        "head = build_classifier()\n",
        "body = build_simple_conv()\n",
        "model = CustomModel(body, head)\n",
        "model.build([None,32,32,3])\n",
        "model.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "nb_nt, nb_t = count_params(model)\n",
        "print(nb_nt)\n",
        "print(nb_t)\n",
        "model.summary()\n",
        "\n",
        "model.freeze_body_weights()\n",
        "\n",
        "nb_nt, nb_t = count_params(model)\n",
        "print(nb_nt)\n",
        "print(nb_t)\n",
        "model.summary()\n",
        "\n",
        "model.unfreeze_body_weights()\n",
        "\n",
        "nb_nt, nb_t = count_params(model)\n",
        "print(nb_nt)\n",
        "print(nb_t)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcMjJexM3PIc"
      },
      "source": [
        "### Tests of different bodies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THpH3vxp3SWC"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "body = build_simple_conv()\n",
        "classifier = build_classifier()\n",
        "model = CustomModel(body,classifier)\n",
        "model.build([None,32,32,3]) \n",
        "model.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "               metrics = ['accuracy'])\n",
        "model.freeze_body_weights()\n",
        "history = model.fit(train_ds, validation_data=val_ds, batch_size=32, epochs = 100, callbacks=[callback])\n",
        "eval = model.evaluate(test_ds, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wERb-N-E3XKb"
      },
      "source": [
        "train_ds = tfds.load(bank, split='train', as_supervised=True)\n",
        "train_ds.map(format_)\n",
        "train_ds, test_ds, validation_ds = tfds.load(bank, split=['test[0%:80%]','test[80%:90%]','test[90%:100%]'],as_supervised=True, batch_size=batch_size)\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "body = build_resnet()\n",
        "classifier = build_classifier()\n",
        "model = CustomModel(body,classifier)\n",
        "model.build([None,32,32,3]) \n",
        "model.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "               metrics = ['accuracy'])\n",
        "history = model.fit(train_ds, validation_data=val_ds, batch_size=32, epochs = 100, callbacks=[callback])\n",
        "eval = model.evaluate(test_ds, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIlGoLlv3Yh8"
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "body = build_WideNet()\n",
        "classifier = build_classifier()\n",
        "model = CustomModel(body,classifier)\n",
        "model.compile(optimizer= keras.optimizers.Adam(),\n",
        "                loss = keras.losses.categorical_crossentropy,\n",
        "               metrics = ['accuracy'])\n",
        "history = model.fit(train_ds, validation_data=val_ds, batch_size=32, epochs = 100, callbacks=[callback])\n",
        "eval = model.evaluate(test_ds, batch_size=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KAreNTvnm5Y"
      },
      "source": [
        "## Tests of WideResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LNytLStnq18"
      },
      "source": [
        "\n",
        "# Load the datasets\n",
        "\n",
        "train_ds, test_ds = tfds.load('cifar10', split=['train','test'],as_supervised=True, batch_size=batch_size)\n",
        "\n",
        "# Format the labeled datasets\n",
        "train_ds = train_ds.map(format_image,  num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(format_image,  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Normalize the labeled datasets\n",
        "train_ds = train_ds.map(normalize_cifar10_element,  num_parallel_calls=AUTOTUNE)\n",
        "test_ds = test_ds.map(normalize_cifar10_element,  num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Format the labeled datasets\n",
        "train_ds = train_ds.map(lambda x,y: from_int_to_categorical(x,y,num_classes))\n",
        "test_ds = test_ds.map(lambda x,y: from_int_to_categorical(x,y,num_classes))\n",
        "\n",
        "# Prefecth the labeled datasets\n",
        "train_ds = train_ds.prefetch(AUTOTUNE)\n",
        "test_ds = test_ds.prefetch(AUTOTUNE)\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "\n",
        "body = build_WideNet()\n",
        "classifier = build_classifier()\n",
        "model = CustomModel(body,classifier)\n",
        "model.build([None,32,32,3])\n",
        "model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001),\n",
        "            loss = keras.losses.categorical_crossentropy,\n",
        "            metrics = ['accuracy'])\n",
        "    \n",
        "history_fit = model.fit(train_ds, validation_data = test_ds, batch_size=32 ,epochs = 100, callbacks=[callback], verbose = 1)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIz9MY9DsEIM"
      },
      "source": [
        "train_ds, test_ds, validation_ds = tfds.load('cifar10', split=['test[0%:80%]','test[80%:90%]','test[90%:100%]'],as_supervised=True, batch_size=batch_size)\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5, mode = \"max\")\n",
        "classifier = build_classifier()\n",
        "model = CustomModel(body,classifier)\n",
        "model.build([None,32,32,3])\n",
        "model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.0005),\n",
        "            loss = keras.losses.sparse_categorical_crossentropy,\n",
        "            metrics = ['accuracy'])\n",
        "model.freeze_body_weights()\n",
        "history_fit = model.fit(train_ds, validation_data=validation_ds,batch_size=32 ,epochs = 100, callbacks=[callback], verbose = 1)  \n",
        "model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}